{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check parallel envs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "vel=1\n",
    "omega =2\n",
    "\n",
    "actions = numpy.expand_dims(numpy.asarray([vel,omega]), axis=1)\n",
    "\n",
    "print(actions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "h=40\n",
    "w=40\n",
    "position_ids = None\n",
    "if position_ids is None:\n",
    "    radius = h\n",
    "    norm_dist = torch.distributions.normal.Normal(0, radius // 4)\n",
    "    x = torch.linspace(-radius, radius, radius + 1)\n",
    "    kern1d = norm_dist.cdf(x)\n",
    "    kern1d = kern1d[1:] - kern1d[:-1]\n",
    "    kern2d = torch.ger(kern1d, kern1d)\n",
    "    position_ids = torch.round(radius**2 * kern2d)\n",
    "    position_ids = position_ids.unsqueeze(0).reshape(1, h, w)\n",
    "    \n",
    "\n",
    "plt.matshow(position_ids.squeeze(0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlnce_baselines.common.aux_losses import AuxLosses\n",
    "# AuxLosses.activate()\n",
    "# AuxLosses.clear()\n",
    "\n",
    "a=1\n",
    "progress_loss=0.01\n",
    "alpha=1.0\n",
    "\n",
    "b=2\n",
    "progress_loss = 0.2\n",
    "\n",
    "\n",
    "if AuxLosses.is_active():\n",
    "    AuxLosses.register_loss(\n",
    "    \"progress_monitor\"+str(b),\n",
    "    progress_loss,\n",
    "    alpha,\n",
    ")\n",
    "    \n",
    "print(AuxLosses._losses)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in AuxLosses._losses.keys():\n",
    "    print(AuxLosses._losses[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "actions = torch.tensor(0)\n",
    "\n",
    "print(torch.rand_like(actions, dtype=torch.float))\n",
    "\n",
    "# actions = torch.where(\n",
    "# torch.rand_like(actions, dtype=torch.float) < beta,\n",
    "#                     batch[\"vln_oracle_action_sensor\"].long(),\n",
    "#                     actions,\n",
    "#                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "with gzip.open('/data/zirshad/VLNCE-data/data/datasets/R2R_VLNCE_v1_preprocessed/val_seen/val_seen.json.gz') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "num_to_actions={0:'Stop',\n",
    "                1:'Move Forward',\n",
    "                2:'Turn Left',\n",
    "                3:'Turn Right',\n",
    "                4:'Look Up',\n",
    "                5:'Look Down'}\n",
    "\n",
    "# print(data['instruction_vocab']['stoi']\n",
    "# )\n",
    "\n",
    "episode = 0\n",
    "\n",
    "\n",
    "ep_id=164\n",
    "print(data['episodes'][ep_id]\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "myFile = np.genfromtxt('/home/zirshad/SASRA/videos/debug/seq2seq_text_attn/run_7/episode=165-ckpt=0-SPL=1.00attention.csv', delimiter=',')\n",
    "actions = np.genfromtxt('/home/zirshad/SASRA/videos/debug/seq2seq_text_attn/run_7/episode=165-ckpt=0-SPL=1.00action.csv', delimiter=',')\n",
    "\n",
    "actions_name =[]\n",
    "print(myFile[0].shape)\n",
    "print(myFile[0])\n",
    "print(actions.shape)\n",
    "\n",
    "for i in range(len(actions)):\n",
    "    actions_name.append(num_to_actions[actions[i]])\n",
    "    \n",
    "instruction = \"CLS \"+ data['episodes'][ep_id]['instruction']['instruction_text'] + \"EOS\"\n",
    "\n",
    "print(actions_name)\n",
    "print(actions)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "cax = ax.matshow(myFile, cmap='bone')\n",
    "fig.colorbar(cax)\n",
    "fig.set_size_inches(20, 20)\n",
    "plt.xticks(np.arange(0, myFile.shape[1], 1.0))\n",
    "plt.yticks(np.arange(0, myFile.shape[0], 1.0))\n",
    "ax.set_xticklabels(instruction.split(\" \"), rotation=90)\n",
    "ax.set_yticklabels(actions_name)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['episodes'][episode]['instruction']['instruction_text']\n",
    ")\n",
    "\n",
    "instruction =data['episodes'][episode]['instruction']['instruction_text'] \n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"Tokenized Episode: \",episode, tokenizer.tokenize(instruction, add_special_tokens = True))\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(instruction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-16 21:14:33,178 Initializing dataset VLN-CE-v1\n",
      "2020-09-16 21:14:33,784 [construct_envs] Using GPU ID 0\n",
      "2020-09-16 21:14:33,787 [construct_envs] Using GPU ID 0\n",
      "2020-09-16 21:14:33,788 [construct_envs] Using GPU ID 0\n",
      "2020-09-16 21:14:33,790 [construct_envs] Using GPU ID 0\n",
      "2020-09-16 21:14:33,791 [construct_envs] Using GPU ID 0\n",
      "2020-09-16 21:14:33,793 Initializing dataset VLN-CE-v1\n",
      "2020-09-16 21:14:34,213 initializing sim Sim-v0\n",
      "I0916 21:14:39.683124 28384 simulator.py:168] Loaded navmesh data/scene_datasets/mp3d/Vvot9Ly1tCj/Vvot9Ly1tCj.navmesh\n",
      "2020-09-16 21:14:39,690 Initializing task VLN-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rgb': array([[[189, 175, 157],\n",
      "        [185, 170, 153],\n",
      "        [159, 143, 126],\n",
      "        ...,\n",
      "        [172, 164, 140],\n",
      "        [175, 159, 148],\n",
      "        [179, 163, 152]],\n",
      "\n",
      "       [[189, 175, 157],\n",
      "        [159, 143, 126],\n",
      "        [168, 152, 135],\n",
      "        ...,\n",
      "        [169, 159, 135],\n",
      "        [179, 163, 152],\n",
      "        [179, 163, 152]],\n",
      "\n",
      "       [[187, 173, 155],\n",
      "        [189, 175, 157],\n",
      "        [189, 174, 157],\n",
      "        ...,\n",
      "        [169, 159, 135],\n",
      "        [180, 163, 152],\n",
      "        [180, 163, 152]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[180, 157, 123],\n",
      "        [178, 155, 121],\n",
      "        [178, 155, 121],\n",
      "        ...,\n",
      "        [173, 164, 141],\n",
      "        [172, 164, 138],\n",
      "        [175, 167, 143]],\n",
      "\n",
      "       [[171, 148, 113],\n",
      "        [170, 146, 112],\n",
      "        [178, 155, 121],\n",
      "        ...,\n",
      "        [172, 164, 140],\n",
      "        [173, 165, 141],\n",
      "        [172, 164, 140]],\n",
      "\n",
      "       [[157, 133,  98],\n",
      "        [170, 146, 112],\n",
      "        [170, 146, 112],\n",
      "        ...,\n",
      "        [172, 164, 140],\n",
      "        [173, 165, 141],\n",
      "        [174, 166, 141]]], dtype=uint8), 'depth': array([[[0.12251721],\n",
      "        [0.12234299],\n",
      "        [0.12216928],\n",
      "        ...,\n",
      "        [0.08898723],\n",
      "        [0.0881846 ],\n",
      "        [0.08739632]],\n",
      "\n",
      "       [[0.12251631],\n",
      "        [0.12234299],\n",
      "        [0.12216928],\n",
      "        ...,\n",
      "        [0.08898864],\n",
      "        [0.08818599],\n",
      "        [0.08739769]],\n",
      "\n",
      "       [[0.12251631],\n",
      "        [0.12234209],\n",
      "        [0.12216928],\n",
      "        ...,\n",
      "        [0.08899053],\n",
      "        [0.08818738],\n",
      "        [0.08739905]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.12589869],\n",
      "        [0.12575808],\n",
      "        [0.12561683],\n",
      "        ...,\n",
      "        [0.09019517],\n",
      "        [0.08938594],\n",
      "        [0.08859111]],\n",
      "\n",
      "       [[0.12576279],\n",
      "        [0.12562153],\n",
      "        [0.1254806 ],\n",
      "        ...,\n",
      "        [0.09018208],\n",
      "        [0.08937309],\n",
      "        [0.08857848]],\n",
      "\n",
      "       [[0.12562624],\n",
      "        [0.12548624],\n",
      "        [0.12534562],\n",
      "        ...,\n",
      "        [0.090169  ],\n",
      "        [0.08936023],\n",
      "        [0.08856585]]], dtype=float32), 'instruction': {'text': 'Pass the couches and go straight until you get to the glass table with fancy chairs then stop. ', 'tokens': [1721, 2389, 595, 119, 1067, 2288, 2524, 2703, 1046, 2438, 2389, 1057, 2342, 2661, 913, 458, 2394, 2278, 15], 'trajectory_id': 3287}, 'vln_oracle_action_sensor': array([2]), 'progress': 0.0, 'heading': array([0.14677796], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "from vlnce_baselines.common.env_utils import (\n",
    "    SimpleRLEnv\n",
    ")\n",
    "\n",
    "from vlnce_baselines.common.env_utils import construct_envs, construct_env\n",
    "from habitat import Config\n",
    "from vlnce_baselines.config.default import get_config\n",
    "from habitat_baselines.common.environments import get_env_class\n",
    "\n",
    "#Import config and perform some manipulation\n",
    "exp_config ='vlnce_baselines/config/paper_configs/seq2seq_robo.yaml'\n",
    "config = get_config(exp_config, None)\n",
    "split = config.TASK_CONFIG.DATASET.SPLIT\n",
    "config.defrost()\n",
    "config.TASK_CONFIG.TASK.NDTW.SPLIT = split\n",
    "config.TASK_CONFIG.TASK.SDTW.SPLIT = split\n",
    "\n",
    "# if doing teacher forcing, don't switch the scene until it is complete\n",
    "if config.DAGGER.P == 1.0:\n",
    "    config.TASK_CONFIG.ENVIRONMENT.ITERATOR_OPTIONS.MAX_SCENE_REPEAT_STEPS = (\n",
    "        -1\n",
    "    )\n",
    "config.freeze()\n",
    "\n",
    "envs = construct_env(config)\n",
    "\n",
    "# print(get_env_class(config.ENV_NAME))\n",
    "# envs = construct_envs(config, get_env_class(config.ENV_NAME))\n",
    "obs=envs.reset()\n",
    "\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb\n",
      "depth\n",
      "instruction\n",
      "vln_oracle_action_sensor\n",
      "progress\n",
      "heading\n",
      "[1721, 2389, 595, 119, 1067, 2288, 2524, 2703, 1046, 2438, 2389, 1057, 2342, 2661, 913, 458, 2394, 2278, 15]\n",
      "[1721, 2389, 595, 119, 1067, 2288, 2524, 2703, 1046, 2438, 2389, 1057, 2342, 2661, 913, 458, 2394, 2278, 15]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6c280f11f608>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'instruction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for k,v in obs.items():\n",
    "    print(k)\n",
    "    \n",
    "print(obs['instruction']['tokens'])\n",
    "\n",
    "ins = obs['instruction']['tokens']\n",
    "print(ins)\n",
    "# print(ins.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "def get_bert_tokens(sentence, max_seq_length, tokenizer):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "    if len(tokens) < max_seq_length:\n",
    "        padded_tokens = tokens + ['[PAD]' for _ in range(max_seq_length - len(tokens))]\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "    return token_ids\n",
    "def transform_obs(\n",
    "    observations: List[Dict], instruction_sensor_uuid: str, is_bert = False, max_seq_length = 200\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    r\"\"\"Extracts instruction tokens from an instruction sensor and\n",
    "    transposes a batch of observation dicts to a dict of batched\n",
    "    observations.\n",
    "    Args:\n",
    "        observations:  list of dicts of observations.\n",
    "        instruction_sensor_uuid: name of the instructoin sensor to\n",
    "            extract from.\n",
    "        device: The torch.device to put the resulting tensors on.\n",
    "            Will not move the tensors if None\n",
    "    Returns:\n",
    "        transposed dict of lists of observations.\n",
    "    \"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    for i in range(len(observations)):\n",
    "        if is_bert:\n",
    "            instruction = observations[i][\n",
    "                instruction_sensor_uuid\n",
    "            ][\"text\"]\n",
    "            token_ids = get_bert_tokens(instruction , max_seq_length, tokenizer)\n",
    "            observations[i][instruction_sensor_uuid] = token_ids\n",
    "            # observations[i][instruction_sensor_uuid] = observations[i][\n",
    "            #     instruction_sensor_uuid\n",
    "            # ][\"tokens\"]\n",
    "        else:\n",
    "            observations[i][instruction_sensor_uuid] = observations[i][\n",
    "                instruction_sensor_uuid\n",
    "            ][\"tokens\"]\n",
    "\n",
    "    return observations\n",
    "\n",
    "import torch\n",
    "from vlnce_baselines.common.utils import transform_obs\n",
    "from habitat_baselines.common.utils import batch_obs\n",
    "\n",
    "device = (\n",
    "    torch.device(\"cuda\", config.TORCH_GPU_ID)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "# print(envs.num_envs)\n",
    "# print(envs.current_episodes()[0].episode_id)\n",
    "observations = envs.reset()\n",
    "# observations[0]['instruction_text'] = 'Go to kitchen'\n",
    "\n",
    "# for k,v in observations.items():\n",
    "#     print(k)\n",
    "# print(observations[0])\n",
    "\n",
    "observations = transform_obs(\n",
    "    observations, config.TASK_CONFIG.TASK.INSTRUCTION_SENSOR_UUID, is_bert=True\n",
    ")\n",
    "\n",
    "\n",
    "# print(observations['instruction'])\n",
    "# print(len(observations['instruction']))\n",
    "# # print(observations)\n",
    "# batch = batch_obs(observations, device=device)\n",
    "\n",
    "# print(batch['rgb'].shape)\n",
    "\n",
    "\n",
    "print(observations['glove_tokens'])\n",
    "\n",
    "print(observations['instruction'])\n",
    "# print(batch['instruction_batch'])\n",
    "\n",
    "\n",
    "# print (batch[\"instruction_batch\"].shape)\n",
    "# lengths = (batch['instruction_batch'] != 0.0).long().sum(dim=1)\n",
    "# print(lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch['ego_sem_map'].shape)\n",
    "observation_space=envs.observation_spaces[0]\n",
    "print(observation_space.spaces[\"ego_sem_map\"].shape[0] // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "embedding_dim = 128\n",
    "\n",
    "def imshow(img):\n",
    "    import cv2\n",
    "    import IPython\n",
    "    _,ret = cv2.imencode('.jpg', img) \n",
    "    i = IPython.display.Image(data=ret)\n",
    "    IPython.display.display(i)\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "hidden_size = config.MODEL.SEM_ATTN_ENCODER.hidden_size\n",
    "from vlnce_baselines.models.encoders.bert_models import BertConfig, BertEmbeddings, BertEncoder, BertPooler\n",
    "bert_config = BertConfig(vocab_size_or_config_json_file=observation_space.spaces[\"ego_sem_map\"].high.max() + 1,\n",
    "                         hidden_size=embedding_dim,\n",
    "                         output_size=hidden_size,\n",
    "                         max_position_embeddings=observation_space.spaces[\n",
    "    \"ego_sem_map\"].shape[0] // 2,\n",
    "    num_attention_heads=4,\n",
    "    num_hidden_layers=2,\n",
    "    intermediate_size=64)\n",
    "\n",
    "ego_sem_input = batch[\"ego_sem_map\"]\n",
    "\n",
    "input_ids = ego_sem_input\n",
    "# print(input_ids.shape)\n",
    "# print(bert_config.vocab_size)\n",
    "word_embeddings = nn.Embedding(\n",
    "    bert_config.vocab_size, bert_config.hidden_size, padding_idx=0).to(device)\n",
    "position_embeddings = nn.Embedding(\n",
    "    bert_config.max_position_embeddings, bert_config.hidden_size).to(device)\n",
    "\n",
    "h, w = input_ids.size(1), input_ids.size(2)\n",
    "\n",
    "position_ids=None\n",
    "if position_ids is None:\n",
    "    # position_ids = torch.arange(\n",
    "    #     h * w, dtype=torch.long, device=input_ids.device)\n",
    "    radius = h\n",
    "    norm_dist = torch.distributions.normal.Normal(0, radius // 4)\n",
    "    x = torch.linspace(-radius, radius, radius + 1,\n",
    "                       device=input_ids.device)\n",
    "    kern1d = norm_dist.cdf(x)\n",
    "    kern1d = kern1d[1:] - kern1d[:-1]\n",
    "    kern2d = torch.ger(kern1d, kern1d)\n",
    "    position_ids = torch.round(radius**2 * kern2d)\n",
    "    position_ids = position_ids.unsqueeze(\n",
    "        0).reshape(1, h, w).expand_as(input_ids)\n",
    "    \n",
    "# print(\"Position IDS\",position_ids.shape)\n",
    "\n",
    "w_embed = word_embeddings(input_ids.long())\n",
    "\n",
    "\n",
    "embeddings = BertEmbeddings(bert_config).to(device)\n",
    "embedding_output = embeddings(input_ids, position_ids=None)\n",
    "\n",
    "embed_flat = embedding_output.permute(0,3,1,2).flatten(2).permute(0, 2, 1)\n",
    "\n",
    "print(embed_flat.shape)\n",
    "\n",
    "encoder = BertEncoder(bert_config).to(device)\n",
    "attention_mask = None\n",
    "if attention_mask is None:\n",
    "    attention_mask = input_ids > 0\n",
    "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "extended_attention_mask = extended_attention_mask.to(dtype=int)  # fp16 compatibility\n",
    "extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "head_mask = None\n",
    "\n",
    "if head_mask is not None:\n",
    "    if head_mask.dim() == 1:\n",
    "        head_mask = head_mask.unsqueeze(0).unsqueeze(\n",
    "            0).unsqueeze(-1).unsqueeze(-1)\n",
    "        head_mask = head_mask.expand(\n",
    "            bert_config.num_hidden_layers, -1, -1, -1, -1)\n",
    "    elif head_mask.dim() == 2:\n",
    "        head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n",
    "    head_mask = head_mask.to(dtype=int)\n",
    "else:\n",
    "    head_mask = [None] * bert_config.num_hidden_layers\n",
    "\n",
    "encoder_outputs = encoder(embedding_output,extended_attention_mask,head_mask=head_mask)\n",
    "\n",
    "sequence_output = encoder_outputs[0]\n",
    "print(\"Sem map encoder output\",sequence_output.shape)\n",
    "\n",
    "pooler = BertPooler(bert_config).to(device)\n",
    "pooled_output = pooler(sequence_output)\n",
    "\n",
    "print(pooled_output.shape)\n",
    "# from vlnce_baselines.models.encoders.visual_networks import RGBDMapSimpleCNN\n",
    "# sem_map_attn_encoder = RGBDMapSimpleCNN(observation_space, config.MODEL.SEM_ATTN_ENCODER).to(device)\n",
    "# sem_map_attn_embedding = sem_map_attn_encoder(batch)\n",
    "# # sem_map_attn_embedding = sem_map_attn_embedding.permute(0,3,1,2)\n",
    "# print(sem_map_attn_embedding.shape)\n",
    "\n",
    "# imshow(img)\n",
    "# import sys\n",
    "# import numpy\n",
    "# print(ego_sem_input[0].cpu().numpy())\n",
    "\n",
    "# head_mask = [None] * bert_config.num_hidden_layers\n",
    "# print(head_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= nn.AdaptiveAvgPool2d((1,1)).cuda()\n",
    "\n",
    "b = a(sequence_output.permute(0,3,1,2))\n",
    "\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = None\n",
    "if attention_mask is None:\n",
    "    attention_mask = input_ids > 0\n",
    "\n",
    "# We create a 3D attention mask from a 2D tensor mask.\n",
    "# Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "# this attention mask is more simple than the triangular masking of causal attention\n",
    "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "print(extended_attention_mask.shape)\n",
    "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "# masked positions, this operation will create a tensor which is 0.0 for\n",
    "# positions we want to attend and -10000.0 for masked positions.\n",
    "# Since we are adding it to the raw scores before the softmax, this is\n",
    "# effectively the same as removing these entirely.\n",
    "extended_attention_mask = extended_attention_mask # fp16 compatibility\n",
    "extended_attention_mask = ~extended_attention_mask\n",
    "\n",
    "\n",
    "print(extended_attention_mask.flatten(2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from habitat_sim.utils.common import d3_40_colors_rgb\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def semantic_to_image(semantic_obs, num_colors):\n",
    "    from habitat_sim.utils.common import d3_40_colors_rgb\n",
    "    semantic_img = Image.new(\"P\", (semantic_obs.shape[1], semantic_obs.shape[0]))\n",
    "    semantic_img.putpalette(d3_40_colors_rgb[:num_colors].flatten())\n",
    "    semantic_img.putdata((semantic_obs.flatten() % 40).astype(np.uint8))\n",
    "    semantic_img = semantic_img.convert(\"RGB\")\n",
    "    return np.array(semantic_img)\n",
    "\n",
    "def display_sample(rgb_obs, semantic_obs, depth_obs, ego_map):\n",
    "    depth_img = Image.fromarray((np.squeeze(depth_obs, axis=2) / 10 * 255).astype(np.uint8), mode=\"L\")\n",
    "\n",
    "    arr = [rgb_obs, semantic_img, depth_img, ego_map]\n",
    "    titles = ['rgb', 'semantic', 'depth', 'ego_map']\n",
    "    plt.figure(figsize=(16 ,8))\n",
    "    for i, data in enumerate(arr):\n",
    "        ax = plt.subplot(1, 4, i+1)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(titles[i])\n",
    "        plt.imshow(data)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "rgb= obs[0]['rgb']\n",
    "depth= obs[0]['depth']\n",
    "\n",
    "ego_map= obs[0]['ego_sem_map']\n",
    "semantic= obs[0]['semantic']\n",
    "\n",
    "object2idx = envs.call_at(0,'get_object2idx')\n",
    "semantic_obs = object2idx[semantic] \n",
    "ego_map = object2idx[ego_map.astype(np.int)]\n",
    "\n",
    "ego_img= semantic_to_image(ego_map)\n",
    "semantic_img = semantic_to_image(semantic_obs)\n",
    "display_sample(rgb, semantic_img, depth, ego_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Dataset, (Discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import config and perform some manipulation\n",
    "from vlnce_baselines.config.default import get_config\n",
    "exp_config ='vlnce_baselines/config/paper_configs/seq2seq_text_attn_robo.yaml'\n",
    "config = get_config(exp_config, None)\n",
    "split = config.TASK_CONFIG.DATASET.SPLIT\n",
    "config.defrost()\n",
    "config.TASK_CONFIG.TASK.NDTW.SPLIT = split\n",
    "config.TASK_CONFIG.TASK.SDTW.SPLIT = split\n",
    "\n",
    "# if doing teacher forcing, don't switch the scene until it is complete\n",
    "if config.DAGGER.P == 1.0:\n",
    "    config.TASK_CONFIG.ENVIRONMENT.ITERATOR_OPTIONS.MAX_SCENE_REPEAT_STEPS = (\n",
    "        -1\n",
    "    )\n",
    "config.freeze()\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "\n",
    "import lmdb\n",
    "import msgpack_numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "\n",
    "class ObservationsDict(dict):\n",
    "    def pin_memory(self):\n",
    "        for k, v in self.items():\n",
    "            self[k] = v.pin_memory()\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Each sample in batch: (\n",
    "            obs,\n",
    "            prev_actions,\n",
    "            oracle_actions,\n",
    "            inflec_weight,\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    def _pad_helper(t, max_len, fill_val=0):\n",
    "        pad_amount = max_len - t.size(0)\n",
    "        if pad_amount == 0:\n",
    "            return t\n",
    "        \n",
    "        pad = torch.full_like(t[0:1], fill_val).expand(pad_amount, *t.size()[1:])\n",
    "        return torch.cat([t, pad], dim=0)\n",
    "\n",
    "    transposed = list(zip(*batch))\n",
    "\n",
    "    observations_batch = list(transposed[0])\n",
    "    prev_actions_batch = list(transposed[1])\n",
    "    corrected_actions_batch = list(transposed[2])\n",
    "    N = len(corrected_actions_batch)\n",
    "    weights_batch = list(transposed[3])\n",
    "    B = len(prev_actions_batch)\n",
    "    new_observations_batch = defaultdict(list)\n",
    "    for sensor in observations_batch[0]:\n",
    "        if sensor == 'instruction':\n",
    "            for bid in range(N):\n",
    "                new_observations_batch[sensor].append(observations_batch[bid][sensor])\n",
    "        else: \n",
    "            for bid in range(B):\n",
    "                new_observations_batch[sensor].append(observations_batch[bid][sensor])\n",
    "\n",
    "    observations_batch = new_observations_batch\n",
    "\n",
    "    max_traj_len = max(ele.size(0) for ele in prev_actions_batch)\n",
    "#     obs_lengths=[]\n",
    "    for bid in range(B):\n",
    "        for sensor in observations_batch:\n",
    "            if sensor == 'instruction':\n",
    "                continue\n",
    "            observations_batch[sensor][bid] = _pad_helper(\n",
    "                observations_batch[sensor][bid], max_traj_len, fill_val=0.0\n",
    "            )\n",
    "#         obs_lengths.append(prev_actions_batch[bid].shape[0])\n",
    "        prev_actions_batch[bid] = _pad_helper(prev_actions_batch[bid], max_traj_len)\n",
    "        corrected_actions_batch[bid] = _pad_helper(\n",
    "            corrected_actions_batch[bid], max_traj_len, fill_val=-1.0\n",
    "        )\n",
    "        weights_batch[bid] = _pad_helper(weights_batch[bid], max_traj_len)\n",
    "\n",
    "    for sensor in observations_batch:\n",
    "        observations_batch[sensor] = torch.stack(observations_batch[sensor], dim=1)\n",
    "        observations_batch[sensor] = observations_batch[sensor].transpose(1,0)\n",
    "        observations_batch[sensor] = observations_batch[sensor].contiguous().view(\n",
    "            -1, *observations_batch[sensor].size()[2:]\n",
    "        )\n",
    "\n",
    "    prev_actions_batch = torch.stack(prev_actions_batch, dim=1)\n",
    "    corrected_actions_batch = torch.stack(corrected_actions_batch, dim=1)\n",
    "    weights_batch = torch.stack(weights_batch, dim=1)\n",
    "    not_done_masks = torch.ones_like(corrected_actions_batch, dtype=torch.float)\n",
    "    not_done_masks[0] = 0\n",
    "    \n",
    "    prev_actions_batch = prev_actions_batch.transpose(1,0)\n",
    "    not_done_masks = not_done_masks.transpose(1,0)\n",
    "    corrected_actions_batch = corrected_actions_batch.transpose(1,0)\n",
    "    weights_batch = weights_batch.transpose(1,0)\n",
    "    \n",
    "    observations_batch = ObservationsDict(observations_batch)\n",
    "\n",
    "    return (\n",
    "        observations_batch,\n",
    "        prev_actions_batch.contiguous().view(-1, 1),\n",
    "        not_done_masks.contiguous().view(-1, 1),\n",
    "        corrected_actions_batch,\n",
    "        weights_batch,\n",
    "    )\n",
    "\n",
    "\n",
    "def _block_shuffle(lst, block_size):\n",
    "    blocks = [lst[i : i + block_size] for i in range(0, len(lst), block_size)]\n",
    "    random.shuffle(blocks)\n",
    "\n",
    "    return [ele for block in blocks for ele in block]\n",
    "\n",
    "\n",
    "class IWTrajectoryDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lmdb_features_dir,\n",
    "        use_iw,\n",
    "        inflection_weight_coef=1.0,\n",
    "        lmdb_map_size=1e9,\n",
    "        batch_size=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lmdb_features_dir = lmdb_features_dir\n",
    "        self.lmdb_map_size = lmdb_map_size\n",
    "        self.preload_size = batch_size * 100\n",
    "        self._preload = []\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if use_iw:\n",
    "            self.inflec_weights = torch.tensor([1.0, inflection_weight_coef])\n",
    "        else:\n",
    "            self.inflec_weights = torch.tensor([1.0, 1.0])\n",
    "\n",
    "        with lmdb.open(\n",
    "            self.lmdb_features_dir,\n",
    "            map_size=int(self.lmdb_map_size),\n",
    "            readonly=True,\n",
    "            lock=False,\n",
    "        ) as lmdb_env:\n",
    "            self.length = lmdb_env.stat()[\"entries\"]\n",
    "\n",
    "    def _load_next(self):\n",
    "        if len(self._preload) == 0:\n",
    "            if len(self.load_ordering) == 0:\n",
    "                raise StopIteration\n",
    "\n",
    "            new_preload = []\n",
    "            lengths = []\n",
    "            with lmdb.open(\n",
    "                self.lmdb_features_dir,\n",
    "                map_size=int(self.lmdb_map_size),\n",
    "                readonly=True,\n",
    "                lock=False,\n",
    "            ) as lmdb_env, lmdb_env.begin(buffers=True) as txn:\n",
    "                for _ in range(self.preload_size):\n",
    "                    if len(self.load_ordering) == 0:\n",
    "                        break\n",
    "\n",
    "                    new_preload.append(\n",
    "                        msgpack_numpy.unpackb(\n",
    "                            txn.get(str(self.load_ordering.pop()).encode()), raw=False\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    lengths.append(len(new_preload[-1][0]))\n",
    "\n",
    "            sort_priority = list(range(len(lengths)))\n",
    "            random.shuffle(sort_priority)\n",
    "\n",
    "            sorted_ordering = list(range(len(lengths)))\n",
    "            sorted_ordering.sort(key=lambda k: (lengths[k], sort_priority[k]))\n",
    "\n",
    "            for idx in _block_shuffle(sorted_ordering, self.batch_size):\n",
    "                self._preload.append(new_preload[idx])\n",
    "\n",
    "        return self._preload.pop()\n",
    "\n",
    "    def __next__(self):\n",
    "        obs, prev_actions, oracle_actions= self._load_next()\n",
    "        instruction_batch = obs['instruction'][0]\n",
    "        instruction_batch = np.expand_dims(instruction_batch, axis=0)\n",
    "        obs['instruction'] = instruction_batch\n",
    "        for k, v in obs.items():\n",
    "            obs[k] = torch.from_numpy(v)\n",
    "\n",
    "        prev_actions = torch.from_numpy(prev_actions)\n",
    "        \n",
    "        \n",
    "        oracle_actions = torch.from_numpy(oracle_actions)\n",
    "\n",
    "        inflections = torch.cat(\n",
    "            [\n",
    "                torch.tensor([1], dtype=torch.long),\n",
    "                (oracle_actions[1:] != oracle_actions[:-1]).long(),\n",
    "            ]\n",
    "        )\n",
    "        return (obs, prev_actions, oracle_actions, self.inflec_weights[inflections])\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            start = 0\n",
    "            end = self.length\n",
    "        else:\n",
    "            per_worker = int(np.ceil(self.length / worker_info.num_workers))\n",
    "\n",
    "            start = per_worker * worker_info.id\n",
    "            end = min(start + per_worker, self.length)\n",
    "\n",
    "        # Reverse so we can use .pop()\n",
    "        self.load_ordering = list(\n",
    "            reversed(_block_shuffle(list(range(start, end)), self.preload_size))\n",
    "        )\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Dataset, pytorch Dataloader, padding and lmdb load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import config and perform some manipulation\n",
    "from vlnce_baselines.config.default import get_config\n",
    "exp_config ='vlnce_baselines/config/paper_configs/seq2seq_robo.yaml'\n",
    "config = get_config(exp_config, None)\n",
    "split = config.TASK_CONFIG.DATASET.SPLIT\n",
    "config.defrost()\n",
    "config.TASK_CONFIG.TASK.NDTW.SPLIT = split\n",
    "config.TASK_CONFIG.TASK.SDTW.SPLIT = split\n",
    "\n",
    "# if doing teacher forcing, don't switch the scene until it is complete\n",
    "if config.DAGGER.P == 1.0:\n",
    "    config.TASK_CONFIG.ENVIRONMENT.ITERATOR_OPTIONS.MAX_SCENE_REPEAT_STEPS = (\n",
    "        -1\n",
    "    )\n",
    "config.freeze()\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "\n",
    "import lmdb\n",
    "import msgpack_numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "class ObservationsDict(dict):\n",
    "    def pin_memory(self):\n",
    "        for k, v in self.items():\n",
    "            self[k] = v.pin_memory()\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Each sample in batch: (\n",
    "            obs,\n",
    "            prev_actions,\n",
    "            oracle_actions,\n",
    "            inflec_weight,\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    def _pad_helper(t, max_len, fill_val=0):\n",
    "        pad_amount = max_len - t.size(0)\n",
    "        if pad_amount == 0:\n",
    "            return t\n",
    "        \n",
    "        pad = torch.full_like(t[0:1], fill_val).expand(pad_amount, *t.size()[1:])\n",
    "        return torch.cat([t, pad], dim=0)\n",
    "    \n",
    "    def _pad_instruction(t, max_len, fill_val=0):\n",
    "        pad_amount = max_len - t.size(1)\n",
    "        if pad_amount == 0:\n",
    "            return t\n",
    "        pad = torch.full_like(t[:,0], fill_val).expand(*t.size()[:1], pad_amount)\n",
    "        return torch.cat([t, pad], dim=1)\n",
    "\n",
    "    transposed = list(zip(*batch))\n",
    "\n",
    "    observations_batch = list(transposed[0])\n",
    "    prev_actions_batch = list(transposed[1])\n",
    "    corrected_actions_batch = list(transposed[2])\n",
    "    N = len(corrected_actions_batch)\n",
    "#     weights_batch = list(transposed[3])\n",
    "    B = len(prev_actions_batch)\n",
    "    new_observations_batch = defaultdict(list)\n",
    "    for sensor in observations_batch[0]:\n",
    "        if sensor == 'instruction':\n",
    "            for bid in range(N):\n",
    "                new_observations_batch[sensor].append(observations_batch[bid][sensor])\n",
    "        else: \n",
    "            for bid in range(B):\n",
    "                new_observations_batch[sensor].append(observations_batch[bid][sensor])\n",
    "\n",
    "    observations_batch = new_observations_batch\n",
    "\n",
    "    max_traj_len = max(ele.size(0) for ele in prev_actions_batch)\n",
    "    max_insr_len = max(ele.size(1) for ele in observations_batch['instruction'])\n",
    "    for bid in range(B):\n",
    "        for sensor in observations_batch:\n",
    "            if sensor == 'instruction':  \n",
    "                observations_batch[sensor][bid] = _pad_instruction(\n",
    "                    observations_batch[sensor][bid], max_insr_len, fill_val=0.0\n",
    "                )\n",
    "                continue\n",
    "            observations_batch[sensor][bid] = _pad_helper(\n",
    "                observations_batch[sensor][bid], max_traj_len, fill_val=0.0\n",
    "            )\n",
    "        prev_actions_batch[bid] = _pad_helper(prev_actions_batch[bid], max_traj_len)\n",
    "        corrected_actions_batch[bid] = _pad_helper(\n",
    "            corrected_actions_batch[bid], max_traj_len, fill_val=0.0\n",
    "        )\n",
    "#         weights_batch[bid] = _pad_helper(weights_batch[bid], max_traj_len)\n",
    "    \n",
    "\n",
    "    for sensor in observations_batch:\n",
    "        observations_batch[sensor] = torch.stack(observations_batch[sensor], dim=1)\n",
    "#         print(observations_batch[sensor].shape)\n",
    "        observations_batch[sensor] = observations_batch[sensor].transpose(1,0)\n",
    "        observations_batch[sensor] = observations_batch[sensor].contiguous().view(\n",
    "            -1, *observations_batch[sensor].size()[2:]\n",
    "        )\n",
    "\n",
    "    prev_actions_batch = torch.stack(prev_actions_batch, dim=1)\n",
    "    corrected_actions_batch = torch.stack(corrected_actions_batch, dim=1)\n",
    "#     weights_batch = torch.stack(weights_batch, dim=1)\n",
    "    not_done_masks = torch.ones_like(corrected_actions_batch, dtype=torch.float)\n",
    "    not_done_masks[0] = 0\n",
    "    \n",
    "    prev_actions_batch = prev_actions_batch.transpose(1,0)\n",
    "    not_done_masks = not_done_masks.transpose(1,0)\n",
    "    corrected_actions_batch = corrected_actions_batch.transpose(1,0)\n",
    "#     weights_batch = weights_batch.transpose(1,0)\n",
    "    \n",
    "    observations_batch = ObservationsDict(observations_batch)\n",
    "\n",
    "    return (\n",
    "        observations_batch,\n",
    "        prev_actions_batch.contiguous().view(-1, 2),\n",
    "        not_done_masks.contiguous().view(-1, 2),\n",
    "        corrected_actions_batch.contiguous().view(-1,2),\n",
    "    )\n",
    "#     return (\n",
    "#         observations_batch,\n",
    "#         prev_actions_batch,\n",
    "#         not_done_masks,\n",
    "#         corrected_actions_batch,\n",
    "#     )\n",
    "\n",
    "\n",
    "def _block_shuffle(lst, block_size):\n",
    "    blocks = [lst[i : i + block_size] for i in range(0, len(lst), block_size)]\n",
    "    random.shuffle(blocks)\n",
    "\n",
    "    return [ele for block in blocks for ele in block]\n",
    "\n",
    "\n",
    "class IWTrajectoryDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lmdb_features_dir,\n",
    "        use_iw,\n",
    "        inflection_weight_coef=1.0,\n",
    "        lmdb_map_size=1e9,\n",
    "        batch_size=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lmdb_features_dir = lmdb_features_dir\n",
    "        self.lmdb_map_size = lmdb_map_size\n",
    "        self.preload_size = batch_size * 100\n",
    "        self._preload = []\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if use_iw:\n",
    "            self.inflec_weights = torch.tensor([1.0, inflection_weight_coef])\n",
    "        else:\n",
    "            self.inflec_weights = torch.tensor([1.0, 1.0])\n",
    "\n",
    "        with lmdb.open(\n",
    "            self.lmdb_features_dir,\n",
    "            map_size=int(self.lmdb_map_size),\n",
    "            readonly=True,\n",
    "            lock=False,\n",
    "        ) as lmdb_env:\n",
    "            self.length = lmdb_env.stat()[\"entries\"]\n",
    "\n",
    "    def _load_next(self):\n",
    "        if len(self._preload) == 0:\n",
    "            if len(self.load_ordering) == 0:\n",
    "                raise StopIteration\n",
    "\n",
    "            new_preload = []\n",
    "            lengths = []\n",
    "            with lmdb.open(\n",
    "                self.lmdb_features_dir,\n",
    "                map_size=int(self.lmdb_map_size),\n",
    "                readonly=True,\n",
    "                lock=False,\n",
    "            ) as lmdb_env, lmdb_env.begin(buffers=True) as txn:\n",
    "                for _ in range(self.preload_size):\n",
    "                    if len(self.load_ordering) == 0:\n",
    "                        break\n",
    "\n",
    "                    new_preload.append(\n",
    "                        msgpack_numpy.unpackb(\n",
    "                            txn.get(str(self.load_ordering.pop()).encode()), raw=False\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    lengths.append(len(new_preload[-1][0]))\n",
    "\n",
    "            sort_priority = list(range(len(lengths)))\n",
    "            random.shuffle(sort_priority)\n",
    "\n",
    "            sorted_ordering = list(range(len(lengths)))\n",
    "            sorted_ordering.sort(key=lambda k: (lengths[k], sort_priority[k]))\n",
    "\n",
    "            for idx in _block_shuffle(sorted_ordering, self.batch_size):\n",
    "                self._preload.append(new_preload[idx])\n",
    "\n",
    "        return self._preload.pop()\n",
    "\n",
    "    def __next__(self):\n",
    "        obs, prev_actions, oracle_actions= self._load_next()\n",
    "        \n",
    "        for k,v in obs.items():\n",
    "            print(k,v.shape)\n",
    "        print(\"-------------\")\n",
    "        \n",
    "        is_bert=False\n",
    "        \n",
    "        if is_bert:            \n",
    "            instruction_batch = obs['instruction'][0]\n",
    "            instruction_batch = np.expand_dims(instruction_batch, axis=0)\n",
    "            obs['instruction'] = instruction_batch\n",
    "            del obs['glove_tokens']\n",
    "        else:\n",
    "            instruction_batch = obs['glove_tokens'][0]\n",
    "            instruction_batch = np.expand_dims(instruction_batch, axis=0)\n",
    "            obs['instruction'] = instruction_batch\n",
    "            del obs['glove_tokens']\n",
    "            \n",
    "        for k, v in obs.items():\n",
    "            obs[k] = torch.from_numpy(v)\n",
    "\n",
    "        prev_actions = torch.from_numpy(prev_actions)\n",
    "        \n",
    "        \n",
    "        oracle_actions = torch.from_numpy(oracle_actions)\n",
    "        return (obs, prev_actions, oracle_actions)\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            start = 0\n",
    "            end = self.length\n",
    "        else:\n",
    "            per_worker = int(np.ceil(self.length / worker_info.num_workers))\n",
    "\n",
    "            start = per_worker * worker_info.id\n",
    "            end = min(start + per_worker, self.length)\n",
    "\n",
    "        # Reverse so we can use .pop()\n",
    "        self.load_ordering = list(\n",
    "            reversed(_block_shuffle(list(range(start, end)), self.preload_size))\n",
    "        )\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmdb_features_dir = '/srv/share3/mirshad7/data/robo-vln/trajectory_dir/seq2seq_robo/trajectories.lmdb'\n",
    "# lmdb_features_dir = '/home/asawaree2/data/trajectory_dir/seq2seq_text_attn/complete/trajectories.lmdb'\n",
    "\n",
    "USE_IW= False\n",
    "\n",
    "dataset = IWTrajectoryDataset(\n",
    "    lmdb_features_dir,\n",
    "    USE_IW,\n",
    "    inflection_weight_coef=1.0,\n",
    "    lmdb_map_size=1e9,\n",
    "    batch_size=5,\n",
    ")\n",
    "diter = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=5,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=False,\n",
    "    drop_last=True,  # drop last batch if smaller\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = (\n",
    "    torch.device(\"cuda\", config.TORCH_GPU_ID)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "observations_batch,prev_actions_batch,not_done_masks,corrected_actions_batch = next(iter(diter))\n",
    "# observations_batch,prev_actions_batch,not_done_masks,corrected_actions_batch, weights_batch = next(iter(diter))\n",
    "# print(observations_batch['depth_features'].shape)\n",
    "# print(prev_actions_batch.shape)\n",
    "observations_batch = {\n",
    "    k: v.to(device=device, non_blocking=True)\n",
    "    for k, v in observations_batch.items()\n",
    "}\n",
    "\n",
    "# rgb_out_dim = observations_batch['rgb'].shape[3]\n",
    "# rgb_out_dim = observations_batch['depth'].shape[3]\n",
    "\n",
    "# prev_actions     = prev_actions.view(T,N)\n",
    "# masks            = masks.view(T,N)\n",
    "# rgb_embedding    = rgb_embedding.view(self.batch_size, max_len, -1, rgb_out_dim, rgb_out_dim)\n",
    "# depth_embedding  = depth_embedding.view(self.batch_size, max_len, -1, depth_out_dim, depth_out_dim)\n",
    "\n",
    "for k, v in observations_batch.items():\n",
    "    print(k,v.shape)\n",
    "    \n",
    "print(prev_actions_batch[:317,:].shape)\n",
    "print(corrected_actions_batch.shape)\n",
    "print(not_done_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observations_batch['instruction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = observations_batch['instruction'].long()\n",
    "\n",
    "print(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# del observations_batch['instruction']\n",
    "for k, v in observations_batch.items():\n",
    "    print(k,v.shape)\n",
    "\n",
    "# print(prev_actions_batch.shape)\n",
    "\n",
    "# print(prev_actions_batch[300:400])\n",
    "# print(\"--------------\")\n",
    "# print(corrected_actions_batch.shape)\n",
    "\n",
    "print(corrected_actions_batch.shape)\n",
    "# T,N,C = corrected_actions_batch.size()\n",
    "\n",
    "# print(T,N,C)\n",
    "\n",
    "# corrected_actions_batch_1 = corrected_actions_batch.contiguous().view(T*N,C)\n",
    "\n",
    "# print(corrected_actions_batch_1.shape)\n",
    "\n",
    "print(corrected_actions_batch)\n",
    "print(prev_actions_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch_tbptt(batch, prev_actions, not_done_masks, corrected_actions, tbptt_steps, split_dim):\n",
    "    new_observations_batch = defaultdict(list)\n",
    "    split_observations_batch = defaultdict()\n",
    "    batch_split=[]\n",
    "    for sensor in batch:\n",
    "        if sensor == 'instruction':\n",
    "            new_observations_batch[sensor] = batch[sensor]\n",
    "            continue\n",
    "        for x in batch[sensor].split(tbptt_steps, dim=split_dim):\n",
    "            new_observations_batch[sensor].append(x)            \n",
    "    for i, (prev_action_split, corrected_action_split, masks_split) in enumerate(\n",
    "            zip(prev_actions.split(tbptt_steps, dim=split_dim), \n",
    "                  corrected_actions.split(tbptt_steps, dim=split_dim), \n",
    "                  not_done_masks.split(tbptt_steps, dim=split_dim))):\n",
    "            for sensor in new_observations_batch:\n",
    "                if sensor == 'instruction':\n",
    "                    split_observations_batch[sensor] = new_observations_batch[sensor]\n",
    "                else:\n",
    "                    split_observations_batch[sensor] = new_observations_batch[sensor][i]\n",
    "            split = (split_observations_batch, prev_action_split, masks_split, corrected_action_split)\n",
    "            split_observations_batch = {}\n",
    "            batch_split.append(split)\n",
    "            \n",
    "    return batch_split\n",
    "\n",
    "split_dim =0\n",
    "batch = observations_batch\n",
    "\n",
    "print(batch['instruction'].shape)\n",
    "tbptt_steps = 100\n",
    "\n",
    "# # for k,v in batch:\n",
    "# #     print(k,v.shape)\n",
    "\n",
    "# batch = observations_batch\n",
    "batch_split = split_batch_tbptt(batch, prev_actions_batch, not_done_masks, corrected_actions_batch, tbptt_steps, split_dim)\n",
    "\n",
    "\n",
    "# split_dim =0\n",
    "\n",
    "# tbptt_steps = 100\n",
    "\n",
    "\n",
    "# # for k,v in batch:\n",
    "# #     print(k,v.shape)\n",
    "\n",
    "# batch = observations_batch\n",
    "# new_observations_batch = defaultdict(list)\n",
    "# for sensor in batch:\n",
    "#     if sensor == 'instruction':\n",
    "#         continue\n",
    "#     for x in batch[sensor].split(tbptt_steps, dim=split_dim):\n",
    "#         new_observations_batch[sensor].append(x)\n",
    "    \n",
    "# for sensor in new_observations_batch:\n",
    "#     print(new_observations_batch[sensor][0].shape)\n",
    "#     print(batch[sensor].shape)\n",
    "        \n",
    "# print(new_observations_batch[0])\n",
    "    \n",
    "\n",
    "# for i, (x_, y_) in enumerate(\n",
    "#     zip(observations_batch['rgb'].split(tbptt_steps, dim=split_dim), prev_actions_batch.split(tbptt_steps, dim=split_dim))\n",
    "# ):\n",
    "#     print(f\"{i} x: {x_.shape}\")\n",
    "#     print(f\"{i} y: {y_.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def repackage_mini_batch(batch):\n",
    "#     split_observations_batch, prev_action_split, not_done_masks, corrected_action_split = batch\n",
    "#     for sensor in split_observations_batch:\n",
    "#         split_observations_batch[sensor] = split_observations_batch[sensor].contiguous().view(\n",
    "#             -1, *split_observations_batch[sensor].size()[2:]\n",
    "#         )\n",
    "#     return (\n",
    "#         split_observations_batch,\n",
    "#         prev_action_split.contiguous().view(-1, 2),\n",
    "#         not_done_masks.contiguous().view(-1, 2),\n",
    "#         corrected_action_split.contiguous().view(-1,2),\n",
    "#     )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "for split in batch_split:\n",
    "    (   observations_batch,\n",
    "        prev_actions_batch,\n",
    "        not_done_masks,\n",
    "        corrected_actions_batch,\n",
    "    ) = split\n",
    "#     print(not_done_masks)\n",
    "#     print(corrected_action_split.contiguous().view(-1,2)!=0)\n",
    "#     print(prev_action_split.contiguous()[0,:])\n",
    "    for k,v in observations_batch.items():\n",
    "        print(k,v.shape)\n",
    "    print(\"---------------------\")\n",
    "#     print(split_observations_batch['rgb'].shape)\n",
    "\n",
    "\n",
    "# print(split_observations_batch['rgb'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observations_batch['instruction'])\n",
    "\n",
    "instructions = observations_batch['instruction'].long()\n",
    "\n",
    "print(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(dataset.length)\n",
    "print(observations_batch['instruction'].shape)\n",
    "# print(observations_batch['ego_sem_map'].shape)\n",
    "print(len(prev_actions_batch))\n",
    "\n",
    "max_traj_len = max(ele.size(0) for ele in prev_actions_batch)\n",
    "print(corrected_actions_batch.size())z\n",
    "print(prev_actions_batch.size())\n",
    "\n",
    "print(corrected_actions_batch.shape)\n",
    "print(weights_batch.shape)\n",
    "\n",
    "print(observations_batch['rgb'].shape)\n",
    "print(observations_batch['depth_features'].shape)\n",
    "\n",
    "T,N = corrected_actions_batch.shape\n",
    "\n",
    "print(not_done_masks.shape)\n",
    "\n",
    "# print(prev_actions_batch.view(T,N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(corrected_actions_batch)\n",
    "# print(observations_batch['instruction'][1])\n",
    "\n",
    "\n",
    "\n",
    "print(observations_batch['rgb_features'].shape)\n",
    "print(observations_batch['depth_features'].shape)\n",
    "\n",
    "lengths = (observations_batch['instruction'] != 0.0).long().sum(dim=1)\n",
    "\n",
    "print(prev_actions_batch.view(T,N).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Actor Critic Agent /Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def load_checkpoint(checkpoint_path, *args, **kwargs) -> Dict:\n",
    "    r\"\"\"Load checkpoint of specified path as a dict.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path: path of target checkpoint\n",
    "        *args: additional positional args\n",
    "        **kwargs: additional keyword args\n",
    "\n",
    "    Returns:\n",
    "        dict containing checkpoint info\n",
    "    \"\"\"\n",
    "    return torch.load(checkpoint_path, *args, **kwargs)\n",
    "\n",
    "ckpt_path = '/data/zirshad/VLNCE-data/data/checkpoints/seq2seq_text_attn/ckpt.24.pth'\n",
    "\n",
    "# from vlnce_baselines.models.cma_policy import CMAPolicy\n",
    "# from vlnce_baselines.models.seq2seq_text_attn import Seq2Seq_Lang_Attn\n",
    "from vlnce_baselines.models.transformer_policy import TransformerPolicy\n",
    "import torch\n",
    "\n",
    "load_from_ckpt = False\n",
    "device = (\n",
    "    torch.device(\"cuda\", config.TORCH_GPU_ID)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "model_config = config.MODEL\n",
    "\n",
    "model_config.defrost()\n",
    "model_config.TORCH_GPU_ID = config.TORCH_GPU_ID\n",
    "model_config.freeze()\n",
    "actor_critic = TransformerPolicy(\n",
    "                observation_space=envs.observation_spaces[0],\n",
    "                action_space=envs.action_spaces[0],\n",
    "                model_config=model_config,\n",
    "                num_processes = config.NUM_PROCESSES,\n",
    "                batch_size=config.DAGGER.BATCH_SIZE,\n",
    "            )\n",
    "actor_critic.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(actor_critic.parameters(), \n",
    "                                         lr=config.MODEL.TRANSFORMER.lr, \n",
    "                                         betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, \n",
    "                                                               patience=config.MODEL.TRANSFORMER.scheduler_patience, \n",
    "                                                               verbose=True, min_lr=1e-6)\n",
    "\n",
    "if load_from_ckpt:\n",
    "    ckpt_dict = load_checkpoint(ckpt_path, map_location=\"cpu\")\n",
    "    actor_critic.load_state_dict(ckpt_dict[\"state_dict\"])\n",
    "    \n",
    "# if config.DAGGER.PRELOAD_LMDB_FEATURES:\n",
    "#     envs.close()\n",
    "#     del envs\n",
    "#     envs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observations_batch['instruction'].shape)\n",
    "print(observations_batch['rgb'].shape[0])\n",
    "\n",
    "print(observations_batch['instruction'][0,:].unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions =[]\n",
    "for i in range(config.DAGGER.BATCH_SIZE):\n",
    "    instruction = observations_batch['instruction'][i,:].unsqueeze(0)\n",
    "    instructions.append(instruction.expand(int(observations_batch['rgb'].shape[0]/config.DAGGER.BATCH_SIZE), instruction.shape[1]).unsqueeze(0))\n",
    "    \n",
    "instructions = torch.cat(instructions, dim=0)\n",
    "\n",
    "instructions = instructions.view(-1, *instructions.size()[2:]).long()\n",
    "\n",
    "print(instructions.shape)\n",
    "\n",
    "# ins = (instructions != 0.0).long().sum(dim=1)\n",
    "\n",
    "# print(ins)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observations_batch['instruction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2504, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "device = (\n",
    "    torch.device(\"cuda\", config.TORCH_GPU_ID)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "def load_embeddings():\n",
    "    \"\"\" Loads word embeddings from a pretrained embeddings file.\n",
    "\n",
    "    PAD: index 0. [0.0, ... 0.0]\n",
    "    UNK: index 1. mean of all R2R word embeddings: [mean_0, ..., mean_n]\n",
    "    why UNK is averaged:\n",
    "        https://groups.google.com/forum/#!searchin/globalvectors/unk|sort:date/globalvectors/9w8ZADXJclA/hRdn4prm-XUJ\n",
    "\n",
    "    Returns:\n",
    "        embeddings tensor of size [num_words x embedding_dim]\n",
    "    \"\"\"\n",
    "    with gzip.open(config.MODEL.INSTRUCTION_ENCODER.embedding_file, \"rt\") as f:\n",
    "        embeddings = torch.tensor(json.load(f))\n",
    "    return embeddings\n",
    "embeddings = load_embeddings()\n",
    "\n",
    "print(embeddings.shape)\n",
    "\n",
    "embedding_layer = nn.Embedding.from_pretrained(\n",
    "    embeddings=load_embeddings(),\n",
    "    freeze=not config.MODEL.INSTRUCTION_ENCODER.fine_tune_embeddings,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# instructions = observations_batch['instruction'].long()\n",
    "# lengths = (instruction != 0.0).long().sum(dim=1)\n",
    "\n",
    "ins = torch.Tensor(ins).long().to(device)\n",
    "# ins = ins.to(device)\n",
    "embedded = embedding_layer(ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0205adbf4d37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print(ins)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(ins.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/habitat/lib/python3.6/site-packages/torch-1.6.0-py3.6-linux-x86_64.egg/torch/tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# All strings are unicode in Python 3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/habitat/lib/python3.6/site-packages/torch-1.6.0-py3.6-linux-x86_64.egg/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_str_intern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/habitat/lib/python3.6/site-packages/torch-1.6.0-py3.6-linux-x86_64.egg/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str_intern\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    349\u001b[0m                     \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                     \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrided\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/habitat/lib/python3.6/site-packages/torch-1.6.0-py3.6-linux-x86_64.egg/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_formatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimag_formatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/habitat/lib/python3.6/site-packages/torch-1.6.0-py3.6-linux-x86_64.egg/torch/_tensor_str.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mnonzero_finite_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_view\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_view\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mtensor_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnonzero_finite_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "# print(ins)\n",
    "# print(ins.shape)\n",
    "print(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Individual Architecture Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config.defrost()\n",
    "config.MODEL.INSTRUCTION_ENCODER.use_pretrained_embeddings=True\n",
    "config.MODEL.INSTRUCTION_ENCODER.is_bert=False\n",
    "config.freeze()\n",
    "\n",
    "\n",
    "# observations_batch = batch\n",
    "import torch.nn as nn\n",
    "# observations_batch= batch\n",
    "device = (\n",
    "    torch.device(\"cuda\", config.TORCH_GPU_ID)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "observations_batch = {\n",
    "    k: v.to(device=device, non_blocking=True)\n",
    "    for k, v in observations_batch.items()\n",
    "}\n",
    "\n",
    "model_config = config.MODEL\n",
    "# from vlnce_baselines.models.encoders.language_encoder import LanguageEncoder\n",
    "from vlnce_baselines.models.encoders.instruction_encoder import InstructionEncoder\n",
    "\n",
    "# instruction_encoder = LanguageEncoder(model_config.INSTRUCTION_ENCODER, device).to(device)\n",
    "instruction_encoder = InstructionEncoder(model_config.INSTRUCTION_ENCODER).to(device)\n",
    "# encoder_output = instruction_encoder(observations_batch)\n",
    "\n",
    "instruction = observations_batch['instruction'].long()\n",
    "encoder_output = instruction_encoder(instruction)\n",
    "\n",
    "# print(\"encoder hidden\",encoder_hidden[0].shape)\n",
    "print(\"encoder output\",encoder_output.shape)\n",
    "# print(encoder_output.permute(1,0,2)[27][0])\n",
    "# print(encoder_output.permute(1,0,2)[22][1])\n",
    "# print(encoder_hidden[0])\n",
    "\n",
    "print(encoder_output[-1])\n",
    "print(encoder_hidden[0])\n",
    "\n",
    "# bidir = model_config.INSTRUCTION_ENCODER.bidirectional\n",
    "\n",
    "# num_directions = 2 if bidir else 1\n",
    "# encoder2decoder = nn.Linear(model_config.INSTRUCTION_ENCODER.hidden_size * num_directions, model_config.INSTRUCTION_ENCODER.hidden_size * num_directions).to(device)\n",
    "# print(encoder_hidden[-1].shape)\n",
    "# h_t = torch.tanh(encoder2decoder(encoder_hidden[0][-1]))\n",
    "# c_t = encoder_hidden[1]\n",
    "# h_t = h_t.unsqueeze(0)\n",
    "\n",
    "# encoder_hidden = (h_t, c_t)\n",
    "\n",
    "# hidden_size = 256\n",
    "# num_layers=1\n",
    "# batch_size = batch[\"instruction\"].shape[0]\n",
    "# h0 = torch.zeros(\n",
    "#     num_layers,\n",
    "#     batch_size,\n",
    "#     hidden_size\n",
    "# )\n",
    "\n",
    "\n",
    "# print(\"h0\",h0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder_hidden[0].shape)\n",
    "print(h_t.shape)\n",
    "print(c_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGB + Depth Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# observations_batch = batch\n",
    "model_config = config.MODEL\n",
    "def create_mask(batchsize, max_length, length,device):\n",
    "    \"\"\"Given the length create a mask given a padded tensor\"\"\"\n",
    "    tensor_mask = torch.zeros(batchsize, max_length, dtype = torch.bool)\n",
    "    for idx, row in enumerate(tensor_mask):\n",
    "        row[:length[idx]] = 1\n",
    "    tensor_mask.unsqueeze_(-1)\n",
    "    return tensor_mask.to(device)\n",
    "\n",
    "from vlnce_baselines.models.encoders.resnet_encoders import (\n",
    "    TorchVisionResNet50,\n",
    "    VlnResnetDepthEncoder,\n",
    ")\n",
    "# observation_space=envs.observation_spaces[0]\n",
    "# action_space=envs.action_spaces[0]\n",
    "\n",
    "observation_space = envs.observation_space\n",
    "action_space = envs.action_space\n",
    "depth_encoder = VlnResnetDepthEncoder(\n",
    "                observation_space,\n",
    "                output_size=model_config.DEPTH_ENCODER.output_size,\n",
    "                checkpoint=model_config.DEPTH_ENCODER.ddppo_checkpoint,\n",
    "                backbone=model_config.DEPTH_ENCODER.backbone,\n",
    "            )\n",
    "rgb_encoder = TorchVisionResNet50(\n",
    "                observation_space, model_config.RGB_ENCODER.output_size, \n",
    "                model_config.RGB_ENCODER.resnet_output_size,\n",
    "                device, \n",
    "            )\n",
    "\n",
    "rgb_encoder = rgb_encoder.to(device)\n",
    "depth_encoder = depth_encoder.to(device)\n",
    "\n",
    "depth_embedding = depth_encoder(observations_batch)\n",
    "print(depth_embedding.shape)\n",
    "rgb_embedding = rgb_encoder(observations_batch)\n",
    "# x = torch.cat([depth_embedding, rgb_embedding], dim=1)\n",
    "\n",
    "print(depth_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(rgb_embedding.shape)\n",
    "\n",
    "rgb_embedding    = rgb_embedding.view(*depth_embedding.size()[:2], *rgb_embedding.size()[1:])\n",
    "                                      \n",
    "print(rgb_embedding.shape)\n",
    "# batch_size = config.DAGGER.BATCH_SIZE\n",
    "# max_len    = int(rgb_embedding.size(0)/batch_size)\n",
    "\n",
    "# rgb_out_dim = rgb_embedding.shape[3]\n",
    "\n",
    "# rgb_embedding_1    = rgb_embedding.view(batch_size, max_len, -1, rgb_out_dim, rgb_out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(batchsize, max_length, length,device):\n",
    "    \"\"\"Given the length create a mask given a padded tensor\"\"\"\n",
    "    tensor_mask = torch.zeros(batchsize, max_length, dtype = torch.bool)\n",
    "    print(tensor_mask.shape)\n",
    "    for idx, row in enumerate(tensor_mask):\n",
    "        row[:length[idx]] = 1\n",
    "    tensor_mask.unsqueeze_(-1)\n",
    "    return tensor_mask.to(device)\n",
    "\n",
    "# not_done_masks = torch.ones(config.NUM_PROCESSES, 1, device=device)\n",
    "\n",
    "# print(not_done_masks)\n",
    "\n",
    "# not_done_masks[0] = 0\n",
    "\n",
    "def mask_hidden(hidden_states, masks):\n",
    "    if isinstance(hidden_states, tuple):\n",
    "        hidden_states = tuple(v * masks for v in hidden_states)\n",
    "    else:\n",
    "        hidden_states = masks * hidden_states\n",
    "\n",
    "    return hidden_states\n",
    "\n",
    "# decoder_hidden = encoder_hidden\n",
    "# # print(decoder_hidden[0])\n",
    "# # print(1-not_done_masks)\n",
    "\n",
    "# done_masks = 1- not_done_masks\n",
    "# # print(not_done_masks.unsqueeze(0))\n",
    "\n",
    "# print(decoder_hidden[0].shape)\n",
    "\n",
    "# print(done_masks.unsqueeze(0).shape)\n",
    "\n",
    "# hidden_states = mask_hidden(decoder_hidden, done_masks.unsqueeze(0))\n",
    "\n",
    "# # print(hidden_states[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from habitat_baselines.rl.models.rnn_state_encoder import RNNStateEncoder\n",
    "rnn_input_size = (\n",
    "    instruction_encoder.output_size\n",
    "    + model_config.DEPTH_ENCODER.output_size\n",
    "    + model_config.RGB_ENCODER.output_size\n",
    ")\n",
    "\n",
    "state_encoder = RNNStateEncoder(\n",
    "    input_size=rnn_input_size,\n",
    "    hidden_size=model_config.STATE_ENCODER.hidden_size,\n",
    "    num_layers=1,\n",
    "    rnn_type=model_config.STATE_ENCODER.rnn_type,\n",
    ")\n",
    "rnn_hidden_states = torch.ones(\n",
    "    state_encoder.num_recurrent_layers,\n",
    "    config.DAGGER.BATCH_SIZE,\n",
    "    config.MODEL.STATE_ENCODER.hidden_size,\n",
    "    device=device,\n",
    ")\n",
    "x = torch.cat([depth_embedding, rgb_embedding], dim=1)\n",
    "\n",
    "n = rnn_hidden_states.size(1)\n",
    "t = int(x.size(0) / n)\n",
    "\n",
    "x = x.view(t, n, x.size(1))\n",
    "masks = not_done_masks[:,0].view(t, n)\n",
    "\n",
    "# steps in sequence which have zero for any agent. Assume t=0 has\n",
    "# a zero in it.\n",
    "has_zeros = (masks[1:] == 0.0).any(dim=-1).nonzero().squeeze().cpu()\n",
    "\n",
    "if has_zeros.dim() == 0:\n",
    "    has_zeros = [has_zeros.item() + 1]  # handle scalar\n",
    "else:\n",
    "    has_zeros = (has_zeros + 1).numpy().tolist()\n",
    "\n",
    "print(has_zeros)\n",
    "\n",
    "has_zeros = [0] + has_zeros + [t]\n",
    "print(has_zeros)\n",
    "\n",
    "for i in range(len(has_zeros) - 1):\n",
    "    # process steps that don't have any zeros in masks together\n",
    "    start_idx = has_zeros[i]\n",
    "    end_idx = has_zeros[i + 1]\n",
    "    \n",
    "    print(start_idx)\n",
    "    print(end_idx)\n",
    "    \n",
    "    \n",
    "print(masks[start_idx].view(1, -1, 1))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corrected_actions_batch==0)\n",
    "mask_action = corrected_actions_batch==0\n",
    "output = torch.ones_like(corrected_actions_batch)\n",
    "print(\"--------------------\")\n",
    "print(output)\n",
    "\n",
    "output = output.masked_fill_(mask_action, 0)\n",
    "\n",
    "\n",
    "print(\"++++++++++++++++++++++++++++++\")\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "prev_actions = torch.zeros(\n",
    "    config.NUM_PROCESSES, 1, device=device, dtype=torch.long\n",
    ")\n",
    "not_done_masks = torch.zeros(config.NUM_PROCESSES, 1, device=device)\n",
    "\n",
    "not_done_masks[0] = 0\n",
    "\n",
    "# Init the RNN state decoder\n",
    "\n",
    "# observations, prev_actions, not_done_masks, corrected_actions, weights= observations_batch, prev_actions_batch.to(device=device, non_blocking=True), not_done_masks.to(device=device, non_blocking=True), corrected_actions_batch.to(device=device, non_blocking=True), weights_batch.to(device=device, non_blocking=True)\n",
    "# T,N = corrected_actions.size()\n",
    "\n",
    "\n",
    "from vlnce_baselines.models.decoder.attn_decoder import Attn_Decoder,Attn_DecoderSequence\n",
    "rnn_input_size = (model_config.DEPTH_ENCODER.output_size\n",
    "    + model_config.RGB_ENCODER.output_size\n",
    ")\n",
    "state_encoder = Attn_DecoderSequence(\n",
    "    attn_model = 'general',\n",
    "    input_size=rnn_input_size,\n",
    "    hidden_size=model_config.STATE_ENCODER.hidden_size,\n",
    "    num_layers=1,\n",
    "    rnn_type=model_config.STATE_ENCODER.rnn_type,\n",
    ").to(device)\n",
    "\n",
    "#RNN Input Size (Seq Length, Batch Size, Input Size)\n",
    "#RNN Hidden Size (Num_layers, Batch Size, Hidden Size)\n",
    "\n",
    "# rnn_hidden_states = torch.ones(\n",
    "#     state_encoder.num_recurrent_layers,\n",
    "#     config.DAGGER.BATCH_SIZE,\n",
    "#     config.MODEL.STATE_ENCODER.hidden_size,\n",
    "#     device=device,\n",
    "# )\n",
    "\n",
    "decoder_hidden = encoder_hidden\n",
    "x = torch.cat([depth_embedding, rgb_embedding], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "num_layers =1\n",
    "rnn_type = model_config.STATE_ENCODER.rnn_type\n",
    "rnn = getattr(nn, rnn_type)(\n",
    "    input_size=rnn_input_size,\n",
    "    hidden_size=model_config.STATE_ENCODER.hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    batch_first = True,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "batch_size = decoder_hidden[0].size(1)\n",
    "\n",
    "N = int(x.size(0)/batch_size)\n",
    "\n",
    "# # unflatten\n",
    "# x = x.view(batch_size, N, -1)\n",
    "\n",
    "# x =x[:,0,:]\n",
    "# x= x.unsqueeze(1)\n",
    "# print(x.shape)\n",
    "\n",
    "# # embed_mask = create_mask(x.shape[0], x.shape[1], obs_lengths, device)\n",
    "# # embed_mask = embed_mask.expand_as(x)\n",
    "# # x = x*embed_mask\n",
    "\n",
    "# # print(x.shape)\n",
    "\n",
    "# # packed_seq = nn.utils.rnn.pack_padded_sequence(x, obs_lengths, batch_first=True, enforce_sorted=False)\n",
    "# output, hidden_states = rnn(x, decoder_hidden)\n",
    "\n",
    "# attn = nn.Linear(model_config.STATE_ENCODER.hidden_size, model_config.STATE_ENCODER.hidden_size, bias=False).to(device)\n",
    "# method ='general'\n",
    "# batch_size, output_len, dimensions = output.size()\n",
    "# max_len = encoder_output.size(1)\n",
    "\n",
    "# if method == \"general\":\n",
    "#     output = output.reshape(batch_size * output_len, dimensions)\n",
    "#     output = attn(output)\n",
    "#     output = output.reshape(batch_size, output_len, dimensions)\n",
    "    \n",
    "# print(output.shape)\n",
    "\n",
    "# print(encoder_output.transpose(1,2).shape)\n",
    "# attention_scores = torch.bmm(output, encoder_output.transpose(1, 2).contiguous())\n",
    "\n",
    "# print(attention_scores.shape)\n",
    "\n",
    "# # print(output.shape)\n",
    "\n",
    "# # print(attention_scores.shape)\n",
    "\n",
    "# # print(attention_scores[1,0,:])\n",
    "\n",
    "# mask = create_mask(encoder_output.shape[0], encoder_output.shape[1], lengths, device)\n",
    "# mask = mask.permute(0,2,1)\n",
    "# mask = mask.expand(encoder_output.shape[0], output_len, encoder_output.shape[1])\n",
    "\n",
    "# # print(mask.shape)\n",
    "\n",
    "# attention_scores.data.masked_fill_(mask == 0, -float('inf'))\n",
    "\n",
    "# print(attention_scores)\n",
    "# # print(attention_scores[1,20,:])\n",
    "\n",
    "# attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "# print(attention_weights)\n",
    "# attention_weights = attention_weights.view(batch_size, output_len, max_len)\n",
    "\n",
    "# print(attention_weights.shape)\n",
    "# print(encoder_output.shape)\n",
    "\n",
    "# context = attention_weights.bmm(encoder_output)\n",
    "\n",
    "# concat_input = torch.cat((output, context), 2)\n",
    "\n",
    "# concat_input = concat_input.contiguous().view(batch_size*N, -1)  # flatten\n",
    "\n",
    "# # print(concat_input.shape)\n",
    "\n",
    "# # print(context.shape)\n",
    "# # print(attention_weights.shape)\n",
    "# # print(output.shape)\n",
    "\n",
    "# concat = nn.Linear(hidden_size * 2, hidden_size).to(device)\n",
    "# print(concat_input.shape)\n",
    "\n",
    "# print(x.size())\n",
    "# print(decoder_hidden[0].size())\n",
    "x, rnn_hidden_states, attn_weights = state_encoder(x, decoder_hidden, encoder_hidden, encoder_output, not_done_masks, lengths, device)\n",
    "\n",
    "# print(x.shape)\n",
    "# print(attn_weights.shape)\n",
    "\n",
    "# print(attn_weights.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(not_done_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_hidden(decoder_hidden, encoder_hidden, masks):\n",
    "    if isinstance(decoder_hidden, tuple):\n",
    "        print(\"tupleeee\")\n",
    "        done_masks = 1-masks\n",
    "        hidden_states = tuple(v * masks + w*done_masks for v,w in zip(decoder_hidden, encoder_hidden))\n",
    "    else:\n",
    "        hidden_states = masks * hidden_states\n",
    "    return hidden_states\n",
    "\n",
    "# print(hidden_states[0])\n",
    "# print(encoder_hidden[0])\n",
    "not_done_masks = torch.ones(config.NUM_PROCESSES, 1, device=device)\n",
    "\n",
    "not_done_masks[0] = 0\n",
    "print(not_done_masks)\n",
    "\n",
    "if not isinstance(hidden_states, tuple):\n",
    "    dec_hidden = self._unpack_hidden(hidden_states)\n",
    "\n",
    "# done_masks = 1- not_done_masks\n",
    "# print(not_done_masks.unsqueeze(0))\n",
    "\n",
    "# if not isinstance(hidden_states, tuple):\n",
    "#     hidden_states = self._unpack_hidden(hidden_states)\n",
    "\n",
    "# enc_hidden = mask_hidden(encoder_hidden, done_masks.unsqueeze(0))\n",
    "# dec_hidden = mask_hidden(hidden_states, not_done_masks.unsqueeze(0))\n",
    "\n",
    "# print(enc_hidden.shape)\n",
    "# print(\"-----------------\")\n",
    "# print(dec_hidden.shape)\n",
    "\n",
    "# print(\"-------------------------\")\n",
    "\n",
    "\n",
    "# hidden_states_final = enc_hidden + dec_hidden\n",
    "\n",
    "# print(hidden_states_final)\n",
    "\n",
    "\n",
    "hidden_states = mask_hidden(dec_hidden, encoder_hidden, not_done_masks.unsqueeze(0))\n",
    "\n",
    "print(dec_hidden)\n",
    "\n",
    "print(\"--------------------\")\n",
    "\n",
    "print(encoder_hidden)\n",
    "\n",
    "print(\"----------------------\")\n",
    "\n",
    "print(hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlnce_baselines.models.decoder.attn_decoder import SoftAttn\n",
    "\n",
    "attn_model = 'general'\n",
    "hidden_size = model_config.STATE_ENCODER.hidden_size\n",
    "decoder_max_len = x.shape[1]\n",
    "# Choose attention model\n",
    "if attn_model != 'none':\n",
    "    attn = SoftAttn(attn_model, hidden_size). to(device)\n",
    "    \n",
    "attn_weights = attn(x, encoder_output,decoder_max_len,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "print(attn_weights.shape)\n",
    "\n",
    "print(attn_weights[3,2,:].sum())\n",
    "\n",
    "concat = nn.Linear(model_config.STATE_ENCODER.hidden_size * 2, model_config.STATE_ENCODER.hidden_size).to(device)\n",
    "\n",
    "context = attn_weights.bmm(encoder_output) # B x S=1 x N\n",
    "print(context.shape)\n",
    "concat_input = torch.cat((x, context), 2)\n",
    "concat_output = F.tanh(concat(concat_input))\n",
    "print(concat_output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "print(rnn_hidden_states.shape)\n",
    "print(encoder_output.shape)\n",
    "print(encoder_hidden[0][-1].shape)\n",
    "print(encoder_hidden[1].shape)\n",
    "\n",
    "print (depth_embedding.shape)\n",
    "print (rgb_embedding.shape)\n",
    "print(x.shape)\n",
    "print(rnn_hidden_states.shape)\n",
    "\n",
    "print(not_done_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from vlnce_baselines.common.aux_losses import AuxLosses\n",
    "recurrent_hidden_states = torch.zeros(\n",
    "    actor_critic.net.num_recurrent_layers,\n",
    "    config.NUM_PROCESSES,\n",
    "    config.MODEL.STATE_ENCODER.hidden_size,\n",
    "    device=device,\n",
    ")\n",
    "prev_actions = torch.zeros(\n",
    "    config.NUM_PROCESSES, 1, device=device, dtype=torch.long\n",
    ")\n",
    "not_done_masks = torch.zeros(config.NUM_PROCESSES, 1, device=device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "            actor_critic.parameters(), lr=config.DAGGER.LR\n",
    "        )\n",
    "\n",
    "for batch in diter:\n",
    "    (\n",
    "                            observations_batch,\n",
    "                            prev_actions_batch,\n",
    "                            not_done_masks,\n",
    "                            corrected_actions_batch,\n",
    "                            weights_batch,\n",
    "                        ) = batch\n",
    "    observations_batch = {\n",
    "                            k: v.to(device=device, non_blocking=True)\n",
    "                            for k, v in observations_batch.items()\n",
    "                        }\n",
    "    observations, prev_actions, not_done_masks, corrected_actions, weights= observations_batch, prev_actions_batch.to(device=device, non_blocking=True), not_done_masks.to(device=device, non_blocking=True), corrected_actions_batch.to(device=device, non_blocking=True), weights_batch.to(device=device, non_blocking=True)\n",
    "    T, N = corrected_actions.size()\n",
    "    \n",
    "    print(T,N)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    recurrent_hidden_states = torch.zeros(\n",
    "        actor_critic.net.num_recurrent_layers,\n",
    "        N,\n",
    "        config.MODEL.STATE_ENCODER.hidden_size,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "\n",
    "    distribution = actor_critic.build_distribution(\n",
    "        observations, recurrent_hidden_states, prev_actions, not_done_masks\n",
    "    )\n",
    "\n",
    "    logits = distribution.logits\n",
    "#     logits = logits.view(T, N, -1)\n",
    "    print(logits)\n",
    "#     print(logits.permute(0, 2, 1).shape)\n",
    "    corrected_actions = corrected_actions.contiguous().view(T*N)\n",
    "    print(corrected_actions)\n",
    "    action_loss = F.cross_entropy(\n",
    "            logits, corrected_actions, reduction=\"none\"\n",
    "        )\n",
    "\n",
    "#     action_loss = F.cross_entropy(\n",
    "#             logits.permute(0, 2, 1), corrected_actions, reduction=\"none\"\n",
    "#         )\n",
    "\n",
    "    action_loss = action_loss.view(T, N)\n",
    "    \n",
    "    print(action_loss.shape)\n",
    "    print(weights_batch.shape)\n",
    "    print(\"------------------\")\n",
    "    action_loss = ((weights * action_loss).sum(0) / weights.sum(0)).mean()\n",
    "    \n",
    "    \n",
    "\n",
    "    loss = action_loss \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    del distribution, logits, loss, action_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recurrent_hidden_states.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from vlnce_baselines.common.utils import transform_obs\n",
    "from habitat_baselines.common.utils import batch_obs\n",
    "\n",
    "observations = envs.reset()\n",
    "observations = transform_obs(\n",
    "    observations, config.TASK_CONFIG.TASK.INSTRUCTION_SENSOR_UUID\n",
    ")\n",
    "batch = batch_obs(observations, device=device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(batch['instruction'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def hook_builder(tgt_tensor):\n",
    "#     def hook(m, i, o):\n",
    "#         tgt_tensor.set_(o.cpu())\n",
    "#     return hook\n",
    "# rgb_features = None\n",
    "# rgb_hook = None\n",
    "# if config.MODEL.RGB_ENCODER.cnn_type == \"TorchVisionResNet50\":\n",
    "#     rgb_features = torch.zeros((1,), device=\"cpu\")\n",
    "#     rgb_hook = actor_critic.net.rgb_encoder.layer_extract.register_forward_hook(\n",
    "#         hook_builder(rgb_features)\n",
    "#     )\n",
    "    \n",
    "# sem_map_attn_features = None\n",
    "# sem_map_attn_hook = None\n",
    "# sem_map_attn_features = torch.zeros((1,), device=\"cpu\")\n",
    "# sem_map_attn_hook = actor_critic.net.sem_map_attn_encoder.ego_sem_model.register_forward_hook(\n",
    "#         hook_builder(sem_map_attn_features)\n",
    "#     )\n",
    "\n",
    "recurrent_hidden_states = torch.zeros(\n",
    "    actor_critic.net.num_recurrent_layers,\n",
    "    config.NUM_PROCESSES,\n",
    "    config.MODEL.STATE_ENCODER.hidden_size,\n",
    "    device=device,\n",
    ")\n",
    "prev_actions = torch.zeros(\n",
    "    config.NUM_PROCESSES, 1, device=device, dtype=torch.long\n",
    ")\n",
    "not_done_masks = torch.zeros(config.NUM_PROCESSES, 1, device=device)\n",
    "'''\n",
    "Check if rgb_features is getting updated\n",
    "'''\n",
    "(_, actions, _, recurrent_hidden_states) = actor_critic.act(\n",
    "    batch,\n",
    "    recurrent_hidden_states,\n",
    "    prev_actions,\n",
    "    not_done_masks,\n",
    "    deterministic=False,\n",
    ")\n",
    "\n",
    "print(actions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(actions.view(-1).shape)\n",
    "print(prev_actions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch['lang_attention'].shape)\n",
    "outputs = envs.step([a[0].item() for a in actions])\n",
    "observations, _, dones, infos = [list(x) for x in zip(*outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observations[2]['rgb'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space= envs.observation_spaces[0]\n",
    "if 'ego_sem_map' in observation_space.spaces:\n",
    "    print('true')\n",
    "    \n",
    "sem_is_color = len(\n",
    "                observation_space.spaces['ego_sem_map'].shape) > 2\n",
    "\n",
    "print(sem_is_color)\n",
    "\n",
    "print(batch[\"ego_sem_map\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prev_actions_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Model Outputs(Instruction Encoder, State encoder, here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from vlnce_baselines.common.aux_losses import AuxLosses\n",
    "import torch.nn as nn\n",
    "\n",
    "observations, prev_actions, not_done_masks, corrected_actions, weights= observations_batch, prev_actions_batch.to(device=device, non_blocking=True), not_done_masks.to(device=device, non_blocking=True), corrected_actions_batch.to(device=device, non_blocking=True), weights_batch.to(device=device, non_blocking=True)\n",
    "T, N = corrected_actions.size()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "prev_actions_list =None\n",
    "AuxLosses.clear()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1, reduction=\"none\")\n",
    "\n",
    "distribution = actor_critic.build_distribution(\n",
    "    observations, prev_actions_list, prev_actions, not_done_masks\n",
    ")\n",
    "\n",
    "\n",
    "logits = distribution.logits\n",
    "\n",
    "\n",
    "\n",
    "corrected_actions = corrected_actions.contiguous().view(T*N)\n",
    "action_loss = criterion(logits, corrected_actions)\n",
    "\n",
    "print(\"action loss\",action_loss.shape)\n",
    "print(\"without view mean\",action_loss.mean())\n",
    "\n",
    "\n",
    "action_loss = action_loss.view(T, N)\n",
    "print(((weights * action_loss).sum(0) / weights.sum(0)).shape)\n",
    "action_loss = ((weights * action_loss).sum(0) / weights.sum(0)).mean()\n",
    "\n",
    "print(\"action_loss\",action_loss)\n",
    "\n",
    "# aux_mask = (weights > 0).view(-1)\n",
    "# aux_loss = AuxLosses.reduce(aux_mask)\n",
    "\n",
    "loss = action_loss \n",
    "# loss.backward()\n",
    "\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits)\n",
    "print(corrected_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config ='vlnce_baselines/config/paper_configs/cma.yaml'\n",
    "config = get_config(exp_config, None)\n",
    "\n",
    "from habitat_baselines.common.baseline_registry import baseline_registry\n",
    "trainer_init = baseline_registry.get_trainer(config.TRAINER_NAME)\n",
    "assert trainer_init is not None, f\"{config.TRAINER_NAME} is not supported\"\n",
    "trainer = trainer_init(config)\n",
    "\n",
    "print(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habitat",
   "language": "python",
   "name": "habitat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
